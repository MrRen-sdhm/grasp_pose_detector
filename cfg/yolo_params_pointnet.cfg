# Path to config file for robot hand geometry
hand_geometry_filename = ../cfg/hand_geometry.cfg

# Pointnet parameters
grasp_points_num = 750
min_point_limit = 50 # not used yet

# Neural network forward device 0:cpu 1:gpu
device = 1

# Batch size for network forward, reduce it if CUDA out of memory.
batch_size = 256

# Path to directory that contains neural network weights
weights_file_gpu = /home/sdhm/Projects/PointNetGPD/PointNetGPD/assets/learned_models/libtorch_gpu.pt
weights_file_cpu = /home/sdhm/Projects/PointNetGPD/PointNetGPD/assets/learned_models/libtorch_cpu.pt

# Path to directory that contains yolo configuration file
yolo_config_filename = /home/sdhm/catkin_ws/src/arm_robot/pickup/data/yolov3.cfg

# Path to directory that contains yolo weights file
yolo_weights_filename = /home/sdhm/catkin_ws/src/arm_robot/pickup/data/yolov3.weights

# Path to rgb image file
rgb_image_filename = /home/sdhm/图片/kinect2_cloud_samples/data/1/0000_color.jpg

# Path to pcd file
pcd_filename = /home/sdhm/图片/kinect2_cloud_samples/data/1/0000_cloud.pcd

# Preprocessing of point cloud
#   voxelize: if the cloud gets voxelized/downsampled
#   remove_outliers: if statistical outliers are removed from the cloud (used to remove noise)
#   workspace: workspace of the robot (dimensions of a cube centered at origin of point cloud)
#   camera_position: position of the camera from which the cloud was taken
#   sample_above_plane: only draws samples which do not belong to the table plane
voxelize = 0
voxel_size = 0.003
remove_outliers = 0
workspace = -1.0 1.0 -1.0 1.0 -1.0 1.0
camera_position = 0 0 0
sample_above_plane = 0

# Grasp candidate generation
#   num_samples: number of samples to be drawn from the point cloud
#   num_threads: number of CPU threads to be used
#   nn_radius: neighborhood search radius for the local reference frame estimation
#   num_orientations: number of robot hand orientations to evaluate
#   num_finger_placements: number of finger placements to evaluate
#   hand_axes: axes about which the point neighborhood gets rotated (0: approach, 1: binormal, 2: axis)
#              (see https://raw.githubusercontent.com/atenpas/gpd2/master/readme/hand_frame.png)
#   deepen_hand: if the hand is pushed forward onto the object
#   friction_coeff: angle of friction cone in degrees
#   min_viable: minimum number of points required on each side to be antipodal
num_samples = 30
num_threads = 4
nn_radius = 0.01
num_orientations = 4
num_finger_placements = 10
hand_axes = 1 2
deepen_hand = 1
friction_coeff = 20
min_viable = 6

# Filtering of candidates
#   min_aperture: the minimum gripper width
#   max_aperture: the maximum gripper width
#   workspace_grasps: dimensions of a cube centered at origin of point cloud; should be smaller than <workspace>
min_aperture = 0.0
max_aperture = 0.085
workspace_grasps = -1.0 1.0 -1.0 1.0 -1.0 1.0

# Filtering of candidates based on their approach direction
#   filter_approach_direction: turn filtering on/off
#   direction: direction to compare against
#   angle_thresh: angle in radians above which grasps are filtered
filter_approach_direction = 0
direction = 1 0 0
thresh_rad = 2.0

# Clustering of grasps
#   min_inliers: minimum number of inliers per cluster; set to 0 to turn off clustering
min_inliers = 0

# Grasp selection
#   num_selected: number of selected grasps (sorted by score)
num_selected = 3

# Visualization
#   plot_normals: plot the surface normals
#   plot_samples: plot the samples
#   plot_candidates: plot the grasp candidates
#   plot_filtered_candidates: plot the grasp candidates which remain after filtering
#   plot_valid_grasps: plot the candidates that are identified as valid grasps
#   plot_clustered_grasps: plot the grasps that after clustering
#   plot_selected_grasps: plot the selected grasps (final output)
plot_normals = 0
plot_samples = 1
plot_candidates = 0
plot_filtered_candidates = 1
plot_valid_grasps = 0
plot_clustered_grasps = 0
plot_selected_grasps = 1
